{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load the PDF document\n",
    "file_path = \"documents/langchain_docs.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "# Split the pages into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=3000, chunk_overlap=500)\n",
    "\n",
    "\n",
    "chunks = text_splitter.split_documents(pages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'documents/langchain_docs.pdf', 'page': 0}, page_content=\"10/13/23, 2:00 PM Using LangSmith to Support Fine-tuning\\nhttps://blog.langchain.dev/using-langsmith-to-support-fine-tuning-of-open-source-llms/ 1/15Using LangSmith to Suppo rt Fine-\\ntuning\\nBY LANGCHAIN 9 MIN READ AUG 23, 2023\\nSummary\\nWe created a guide for fine-tuning and evaluating LLMs using LangSmith for\\ndataset management and evaluation. We did this both with an open source\\nLLM on CoLab and HuggingFace for model training, as well as OpenAI's new\\nfinetuning service. As a test case, we fine-tuned LLaMA2-7b-chat and gpt-3.5-\\nSubscribe\"),\n",
       " Document(metadata={'source': 'documents/langchain_docs.pdf', 'page': 1}, page_content=\"10/13/23, 2:00 PM Using LangSmith to Support Fine-tuning\\nhttps://blog.langchain.dev/using-langsmith-to-support-fine-tuning-of-open-source-llms/ 2/15turbo for an extraction task (knowledge graph triple extraction) using training\\ndata exported from LangSmith and also evaluated the results using LangSmith.\\nThe CoLab guide is here.\\nContext\\nInterest in fine-tuning has grown rapidly over the past few weeks. This can largely\\nbe attributed to two causes.\\nFirst, the open source LLM ecosystem has grown remarkably, progressing from\\nopen source LLMs that lagged the state-of-the-art (SOTA) by a wide margin to\\nnear-SOTA (e.g., Llama-2) LLMs that can be run on consumer laptops in ~1 year!\\nThe drivers of this progress include increasingly large corpus of training data\\n(x-axis, below) and fine-tuning (y-axis) for instruction-following and better-\\nhuman-aligned responses. Performant open source base models o\\x00er benefits\\nsuch as cost savings (e.g., for token-intensive tasks), privacy, and - with fine\\ntuning - the opportunity to exceed SOTA LLMs with much smaller open source for\\nhighly specific tasks.\\nSecond, the leading LLM provider, OpenAI, has released fine-tuning support for\\ngpt-3.5-turbo (and other models). Previously, fine-tuning was only available\\nfor older models. These models were not nearly as capable as newer models,\\nmeaning even a\\x00er fine-tuning, they o\\x00en weren't competitive with GPT-3.5 and\\nfew-shot examples. Now that newer models can be fine-tuned, many expect this\\nto change.\\nSome have argued that organizations may opt for many specialist fine-tuned\\nLLMs derived from open source base models over a single massive generalist\\nmodel. With this and libraries such as HuggingFace to support fine-tuning inLANGCHAIN BLOG\"),\n",
       " Document(metadata={'source': 'documents/langchain_docs.pdf', 'page': 2}, page_content=\"10/13/23, 2:00 PM Using LangSmith to Support Fine-tuning\\nhttps://blog.langchain.dev/using-langsmith-to-support-fine-tuning-of-open-source-llms/ 3/15mind, you may be curious about when and how to fine-tune. This guide provides\\nan overview and shows how LangSmith can support the process.\\nWhen to fine-tune\\nLLMs can learn new knowledge in at least two ways: weight updates (e.g., pre-\\ntraining or fine-tuning) or prompting (e.g., retrieval augmented generation, RAG).\\nModel weights are like long-term memory whereas the prompt is like short-term\\nmemory. This OpenAI cookbook has a useful analogy: When you fine-tune a\\nmodel, it's like studying for an exam one week away. When you insert knowledge\\ninto the prompt (e.g., via retrieval), it's like taking an exam with open notes.\\nWith this in mind, fine-tuning is not advised for teaching an LLM new knowledge\\nor factual recall; this talk from John Schulman of OpenAI notes that fine-tuning\\ncan increase hallucinations. Fine-tuning is better suited for teaching specialized\\ntasks, but it sho uld be considered relative to prompting or RAG. As discussed\\nhere, fine-tuning can be helpful for well-defined tasks with ample examples and /\\nor LLMs that lack the in-context learning capacity for few-shot prompting. This\\nAnyscale blog summarizes these points well: fine-tuning is for form, not facts.\"),\n",
       " Document(metadata={'source': 'documents/langchain_docs.pdf', 'page': 3}, page_content='10/13/23, 2:00 PM Using LangSmith to Support Fine-tuning\\nhttps://blog.langchain.dev/using-langsmith-to-support-fine-tuning-of-open-source-llms/ 4/15How to fine-tune\\nThere ºs a number of helpful LLaMA fine-tuning recipes that have been released\\nfor tasks such as chat using a subset of the OpenAssistant corpus in\\nHuggingFace. Notably, these work on a single CoLab GPU, which makes the\\nworkflow accessible. However, two of the largest remaining pain points in fine-\\ntuning are dataset collection / cleaning and evaluation. Below we show how\\nLangSmith can be used to help address both of these (green, below).\\nTask\\nTasks like classification / tagging or extraction are well-suited for fine-tuning;\\nAnyscale reported promising results (exceeding GPT4) by fine-tuning LLaMA 7B\\nand 13B LLMs on extraction, text-to-SQL, and QA. As a test case, we chose\\nextraction of knowledge triples of the form (subject, relation, object) from text:\\nthe subject and object are entities and the relation is a property or connection\\nbetween them. Triples can then be used to build knowledge graphs, databases\\nthat store information about entities and their relationships. We built an public\\nStreamlit app for triple extraction from user input text to explore the capacity of\\nLLMs (GPT3.5 or 4) to extract triples with function calling. In parallel, we fine-\\ntuned LLaMA2-7b-chat and GPT-3.5 for this task using public datasets.'),\n",
       " Document(metadata={'source': 'documents/langchain_docs.pdf', 'page': 4}, page_content='10/13/23, 2:00 PM Using LangSmith to Support Fine-tuning\\nhttps://blog.langchain.dev/using-langsmith-to-support-fine-tuning-of-open-source-llms/ 5/15Dataset\\nDataset collection and cleaning is o\\x00en a challenging task in training LLMs.\\nWhen a project is set up in LangSmith, generations are automatically logged,\\nmaking it easy to get a large quantity of data. LangSmith o\\x00ers a queryable\\ninterface so you can use user feedback filters, tags, and other metrics to select\\nfor poor quality cases, correct the outputs in the app, and save to datasets that\\nyou can use to improve the results of your model (see below).\\nAs an example, we created LangSmith train and test datasets from knowledge\\ngraph triples in the public BenchIE and CarbIE datasets; we converted them to a\\nshared JSON format with each triplet represented as {s: subject, object: object,\\nrelation: relationship} and randomly split the combined data to into a  train set of\\n~1500 labeled sentences and a test set of 100 sentences. The CoLab shows how\\nto easily load LangSmith datasets. Once loaded, we create instructions for fine-\\ntuning using the system prompt below and LLaMA instruction tokens (as done\\nhere):\\n\"you are a model tasked with extracting knowledge graph triples from \\n\"The triples consist of:\\\\n\"\\n\"- \\\\\"s\\\\\": the subject, which is the main entity the statement is abou'),\n",
       " Document(metadata={'source': 'documents/langchain_docs.pdf', 'page': 5}, page_content='10/13/23, 2:00 PM Using LangSmith to Support Fine-tuning\\nhttps://blog.langchain.dev/using-langsmith-to-support-fine-tuning-of-open-source-llms/ 6/15\"- \\\\\"object\\\\\": the object, which is the entity or concept that the su\\n\"- \\\\\"relation\\\\\": the relationship between the subject and the object\\n\"Given an input sentence, output the triples that represent the know\\nQuantization\\nAs shown in the excellent guide here, we fine-tune a 7B parameter LLaMA chat\\nmodel. We want to do this on a single GPU (HuggingFace guide here), which\\npresents a challenge: if each parameter is 32 bits, a 7B parameter LLaMA2 model\\nwill occupy 28GB, which exceeds the VRAM of a T4 (16GB). To address this, we\\nquantize the model parameters, which means binning the values (e.g., to 16\\nvalues in the case of 4 bit quantization), which reduces the memory required to\\nstore the model (7B * 4 bits / parameter =3.5GB) ~8-fold.\\nLoRA and qLoRA\\nWith the model in memory, we still need a way to fine-tune within the constraint\\nof the remaining GPU resources. For this, parameter-e\\x00icient fine-tuning (PEFT)\\nis a common approach: LoRA freezes the pre-trained model weights and injects\\ntrainable rank decomposition matrices into each layer of the model architecture\\n(see here), reducing the number of trainable parameters for fine-tuning (e.g.,\\n~1% of the total model). qLoRA extends this by freezing quantized weights.\\nDuring fine-tuning frozen weights are de-quantized for forward and backward\\npasses, but only the (small set of) LoRA adapters are saved in memory, reducing\\nfine-tuned model footprint.\\nTraining'),\n",
       " Document(metadata={'source': 'documents/langchain_docs.pdf', 'page': 6}, page_content='10/13/23, 2:00 PM Using LangSmith to Support Fine-tuning\\nhttps://blog.langchain.dev/using-langsmith-to-support-fine-tuning-of-open-source-llms/ 7/15We started with a pre-trained LLaMA-7b chat model llama-2-7b-chat-hf and\\nfine-tuned on the ~1500 instructions in CoLab on an A100. For training\\nconfiguration, we used LLaMA fine-tuning parameters from here: BitsAndBytes\\nloads the base model with 4-bit precision but forward and backward passes are\\nin fp16. We use Supervised Fine-Tuning (SFT) for fine-tuning on our instructions,\\nwhich is quite fast (< 15 minutes) on an A100 for this small data volume.\\nOpenAI Finetuning\\nTo fine-tune OpenAI\\'s GPT-3.5-turbo chat model, we selected 50 examples from\\nthe training dataset and converted them to a list of chat messages in the\\nexpected format:\\n{\\n    \"messages\": [\\n        {\\n            \"role\": \"user\", \\n            \"content\": \"Extract triplets from the following sentence\\n        {\\n            \"role\": \"assistant\", \\n            \"content\": \"{triples}\"\\n        },\\n        ...\\n    ]\\n}\\nSince the base model is so broadly capable, we don\\'t need much data to achieve\\nthe desired behavior. The training data is meant to steer the model to always\\ngenerate the correct format and style rather than to teach it substantial'),\n",
       " Document(metadata={'source': 'documents/langchain_docs.pdf', 'page': 7}, page_content='10/13/23, 2:00 PM Using LangSmith to Support Fine-tuning\\nhttps://blog.langchain.dev/using-langsmith-to-support-fine-tuning-of-open-source-llms/ 8/15information.  As we will see in the evaluation section below, the 50 training\\nexamples were su\\x00icient to get the model to predict triplets in the correct format\\neach time.\\nWe uploaded the fine-tuning data to via the openai SDK and used the resulting\\nmodel directly in LangChain\\'s ChatOpenAI class. The fine-tuned model can be\\nused directly:\\nfrom langchain.chat_models import ChatOpenAI\\nllm = ChatOpenAI(model=\"ft:gpt-3.5-turbo-0613:{openaiOrg}::{modelId}\\nThe whole process took only a few minutes and required no code changes in our\\nchain, apart from removing the need to add in few-shot examples in the prompt\\ntemplate. We then benchmarked this model against the others to quantify its\\nperformance. You can see an example of the whole process in our CoLab\\nnotebook.\\nEvaluation\\nWe evaluated each model using LangSmith, applying an LLM (GPT-4) evaluator to\\ngrade each prediction, which is instructed to identify factual discrepancies\\nbetween the labels and the predicted triplets. This penalized results when it\\npredicts triplets that are not present in the label or when the prediction fails to\\ninclude a triplet, but it will be lenient if the exact wording of the object or relation\\ndi\\x00ers in a non-meaningful way. The evaluator grades results on a scale from 0-\\n100. We ran the evaluations in CoLab to easily configure our custom evaluator\\nand chains to test.'),\n",
       " Document(metadata={'source': 'documents/langchain_docs.pdf', 'page': 8}, page_content='10/13/23, 2:00 PM Using LangSmith to Support Fine-tuning\\nhttps://blog.langchain.dev/using-langsmith-to-support-fine-tuning-of-open-source-llms/ 9/15The table below shows the evaluation results for the llama base chat model and\\nthe fine-tuned variant. For comparison, we also benchmarked 3 chains using\\nOpenAI ºs chat models: gpt-3.5-turbo using a few-shot prompt, a gpt-3.5-\\nturbo model fine-tuned on 50 training data points, and a few-shot gpt-4 chain:\\nFine-tuned\\nllama-2Base llama-2 chat\\nmodelfew-shot gpt-\\n3.5-turbofinetuned gpt-3.5-\\nturbofew-shot\\ngpt-4\\nScore 49% 38% 40% 56% 59%\\nAnalysis\\nWe use LangSmith to characterize common failure modes and better understand\\nwhere the fine-tuned model behaves better than the base model. By filtering for\\nlow-scored predictions, we can see cases where baseline LLaMA-7b-chat\\ncomically hallucinates:  in the below case the LLM thinks that the subject is\\nHomer Simpson and casually answers outside the scope of the desired format\\n(here). The fine-tuned LLaMA-7b-chat (here) gets much closer to the reference\\n(ground truth).Few-shot prompting of GPT-4 performs the best\\nFine-tuned GPT-3.5 is the runner up\\nFine-tuned LLaMA-7b-chat exceeds the performance of GPT-3.5\\nAnd fine-tuned LLaMA-7b-chat is ~29% better than baseline LLaMA-7b-chat'),\n",
       " Document(metadata={'source': 'documents/langchain_docs.pdf', 'page': 9}, page_content='10/13/23, 2:00 PM Using LangSmith to Support Fine-tuning\\nhttps://blog.langchain.dev/using-langsmith-to-support-fine-tuning-of-open-source-llms/ 10/15\\nExample of the base model hallucinating by inferring too much information from its pretrained\\nknowledge.\\nEven with few-shot examples, the baseline LLaMA-7b-chat has a tendency to\\nanswer in an informal chit-chat style (here).\\nThe base llama model o\\x00en injects narrative even when not asked.\\nIn contrast, the fine-tuned model tends to generate text (here) that is aligned\\nwith the desired answer format and refrains from adding unwanted dialog.'),\n",
       " Document(metadata={'source': 'documents/langchain_docs.pdf', 'page': 10}, page_content='10/13/23, 2:00 PM Using LangSmith to Support Fine-tuning\\nhttps://blog.langchain.dev/using-langsmith-to-support-fine-tuning-of-open-source-llms/ 11/15\\nThe fine-tuned model still had instances where it failed to generate the desired\\ncontent. In the example below (link) the model repeated the instructions instead\\nof generating the extracted results. This could be fixed with various approach\\nsuch as di\\x00erent decoding parameters (or using logit biasing), more instructions,\\na larger base model (e.g., 13b), or improved instructions (prompt engineering).'),\n",
       " Document(metadata={'source': 'documents/langchain_docs.pdf', 'page': 11}, page_content='10/13/23, 2:00 PM Using LangSmith to Support Fine-tuning\\nhttps://blog.langchain.dev/using-langsmith-to-support-fine-tuning-of-open-source-llms/ 12/15\\nThe fine-tuned model occasionally become repetitive.\\nWe can contrast the above few-shot prompted GPT-4 below (link), which is able\\nto extract reasonable triplets without fine-tuning on the training dataset.'),\n",
       " Document(metadata={'source': 'documents/langchain_docs.pdf', 'page': 12}, page_content='10/13/23, 2:00 PM Using LangSmith to Support Fine-tuning\\nhttps://blog.langchain.dev/using-langsmith-to-support-fine-tuning-of-open-source-llms/ 13/15\\nConclusion\\nWe can distill a few central lessons:\\nLangSmith can help address pain points in the fine-tuning workflow,\\nsuch as data collection, evaluation, and inspection of results. We show\\nhow LangSmith makes it easy to collect and load datasets as well as run\\nevaluations and inspect specific generations.\\nRAG or few-sho t prompting sho uld be considered carefully before\\njumping to more challenging and costly fine-tuning. Few-shot prompting\\nGPT-4 actually performs better than all of our fine-tuned variants.\\nFine-tuning small open source models on well-defined tasks can\\noutperform much larger generalist models. Just as Anyscale and other'),\n",
       " Document(metadata={'source': 'documents/langchain_docs.pdf', 'page': 13}, page_content='10/13/23, 2:00 PM Using LangSmith to Support Fine-tuning\\nhttps://blog.langchain.dev/using-langsmith-to-support-fine-tuning-of-open-source-llms/ 14/15Overall, these results (and the linked CoLab) provide a quick recipe for fine-\\ntuning open source LLMs using LangSmith a tool to help with the full workflow.\\nTAGS\\nBy LangChain\\nYOU MIGHT ALSO LIKEhave reported previously, we see that a fine-tuned LLaMA2-chat-7B model\\nexceeds the performance of a much larger generalist LLM (GPT-3.5-turbo).\\nThere are numerous levers to improve fine-tuning performance, most\\nnotably careful task definition and dataset curation. Some works, such as\\nLIMA, have reported impressive performance by fine-tuning LLaMA on as\\nfew as 1000 instruction that were selected for high quality. Further data\\ncollection / cleaning, using a larger base model (e.g., LLaMA 13B), and\\nscaling up with GPU services for fine-tuning (Lambda Labs, Modal, Vast.ai,\\nMosaic, Anyscale, etc) are all foreseeable ways to improve these results.\\n[Week of 10/2] LangChain\\nRelease Notes\\n3 MIN READ BY LANGCHAIN[Week of 9/18] LangChain\\nRelease Notes\\n4 MIN READ BY LANGCHAIN\\n[Week of 8/21] LangChain\\nRelease Notes\\n3 MIN READ RELEASE NOTESMaking Data Ingestion\\nProduction Ready: a\\nLangChain-Powered\\nAirbyte Destination\\n3 MIN READ'),\n",
       " Document(metadata={'source': 'documents/langchain_docs.pdf', 'page': 14}, page_content='10/13/23, 2:00 PM Using LangSmith to Support Fine-tuning\\nhttps://blog.langchain.dev/using-langsmith-to-support-fine-tuning-of-open-source-llms/ 15/15Sign up\\n¬© LangChain Blog 2023 - Powered by Ghost\\nConversational Retrieval\\nAgents\\n4 MIN READLangChain Expression\\nLanguage\\n5 MIN READ')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "# Azure OpenAI configuration\n",
    "token = os.getenv(\"GITHUB_TOKEN\")\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"gpt-4o-mini\"\n",
    "\n",
    "# Initialize the ChatOpenAI LLM\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=token,\n",
    "    openai_api_base=endpoint,\n",
    "    model_name=model_name,\n",
    "    temperature=1.0,\n",
    "    max_tokens=4000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Documents\n",
    "file_path = \"documents/langchain_docs.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "data = loader.load()\n",
    "\n",
    "# 2. Split Documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=3000, chunk_overlap=500)\n",
    "chunks = text_splitter.split_documents(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Store Chunks\n",
    "\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    model=model_name,\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=token\n",
    ")\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents=chunks, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 4. Retrieve Relevant Chunks\n",
    "# (No additional code needed, the retriever is ready to use)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents.stuff import create_stuff_documents_chain \n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# 5. Generate Responses\n",
    "system_prompt = \"\"\"\n",
    "Use the following pieces of retrieved context to answer the user's question. \n",
    "If the context doesn't contain any relevant information to answer the question, say \"I don't know\".\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Answer the question:\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_template(system_prompt)\n",
    "\n",
    "# Create the StuffDocumentsChain\n",
    "question_answer_chain = create_stuff_documents_chain(llm=llm, prompt=qa_prompt)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith is a tool designed to support the fine-tuning and evaluation of large language models (LLMs). It aids in dataset management and evaluation processes, making it easier to collect, clean, and inspect data as well as evaluate the performance of fine-tuned models. LangSmith offers features like automatic logging of generations, a queryable interface for data selection, and assistance in running evaluations, thereby addressing common pain points in the fine-tuning workflow.\n"
     ]
    }
   ],
   "source": [
    "# Invoke the chain with a user's question and the retrieved chunks\n",
    "result = question_answer_chain.invoke({\n",
    "    \"question\": \"what is langsmith?\",\n",
    "    \"context\": chunks\n",
    "})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
